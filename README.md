Whisper Answer Assistant ‚Äì Single README
Captures real-time English transcripts from the microphone; if the sentence is an English question, it instantly generates a short, speakable English answer from ChatGPT.
‚ú® Features
WebRTC VAD for segment-based listening
`faster-whisper` ASR (automatic language detection; forwards if EN)
English question detection
Short, speakable English answer (streamed)
Optional: Copy answer to clipboard
Optional strict filters: minimum English language confidence, minimum characters
Optional answer only technical questions (keyword list)
‚úÖ Requirements
Python 3.9+
macOS / Windows / Linux with microphone access
.env containing OpenAI API key
üõ† Installation
python -m venv .venv‚Ä®source .venv/bin/activate      # Windows: .venv\Scripts\Activate.ps1‚Ä®pip install -e .‚Ä®‚Ä®cp .env.example .env           # add OPENAI_API_KEY=... inside‚Ä®# (optional) Add technical terms to configs/keywords.en.txt (one per line)
‚ñ∂Ô∏è Running
waa --list-devices‚Ä®waa --whisper-model base --device <INDEX>‚Ä®‚Ä®# If PATH conflicts occur (e.g., micromamba), use module or full path:‚Ä®python -m waa.cli --whisper-model base --device <INDEX>‚Ä®# or‚Ä®./.venv/bin/waa --whisper-model base --device <INDEX>
‚Ä¢ If you don‚Äôt provide --device, the system default input device will be used.
‚Ä¢ Recommended models: tiny (fastest), base (balanced), small/medium (more accurate).
‚öôÔ∏è Settings (single location)
All settings are in configs/settings.yaml:
audio:‚Ä®  sample_rate: 16000‚Ä®  frame_ms: 20‚Ä®  vad_aggressiveness: 2‚Ä®  max_segment_sec: 15‚Ä®  silence_follow_sec: 0.6‚Ä®‚Ä®assistant:‚Ä®  auto_copy: false‚Ä®  openai_model: gpt-4o-mini‚Ä®‚Ä®  # Answer only if technical keyword is present:‚Ä®  # (If TRUE, only technical questions will get answers)‚Ä®  require_dev_keyword: false‚Ä®‚Ä®  # Optional stricter filters:‚Ä®  # - Whisper EN language probability threshold‚Ä®  # - Ignore very short questions‚Ä®  min_lang_prob: 0.75‚Ä®  min_chars: 12
üîß ‚ÄúOnly technical questions‚Äù mode
If you set require_dev_keyword: true, the app will send only English questions containing at least one term from the following file to ChatGPT:
configs/keywords.en.txt (example):
api‚Ä®sdk‚Ä®python‚Ä®docker‚Ä®kubernetes‚Ä®sql‚Ä®postgres‚Ä®mongodb‚Ä®redis‚Ä®kafka‚Ä®llm‚Ä®rag‚Ä®embedding‚Ä®security‚Ä®oauth‚Ä®jwt‚Ä®latency‚Ä®scalability‚Ä®throughput
üß™ Expected output
For each completed sentence, you‚Äôll see a transcript line. If the sentence is an English question, the following flow will appear:
üß© English question detected:‚Ä®> <question>‚Ä®ü§ñ Suggested answer (speak this):‚Ä®<streamed answer...>
üßØ Troubleshooting (at a glance)
Invalid number of channels ‚Üí You selected an output-only device. From --list-devices output, choose a microphone with (1 in, 0 out).
PATH/micromamba conflicts ‚Üí Use python -m waa.cli ... or ./.venv/bin/waa ....
`webrtcvad` pkg_resources warning ‚Üí Harmless, can be ignored.
macOS mic permission ‚Üí System Settings ‚Üí Privacy & Security ‚Üí Microphone ‚Üí Allow Terminal/IDE.
Linux ‚Üí For PortAudio: sudo apt-get install portaudio19-dev.
Windows ‚Üí Make sure VC++ Build Tools are installed if needed.
üîå (Optional) Toggle on/off with hotkey
If you don‚Äôt want answers when you are speaking, and want to enable it only when the other party is speaking:
pip install pynput
In src/waa/app.py (top of file) add:
from pynput import keyboard‚Ä®ARMED = True‚Ä®def on_press(key):‚Ä®    global ARMED‚Ä®    try:‚Ä®        if key == keyboard.Key.f8:‚Ä®            ARMED = not ARMED‚Ä®            print(f"\n[Toggle] Answer routing: {'ON' if ARMED else 'OFF'}")‚Ä®    except Exception:‚Ä®        pass
At the start of run() open the listener:
listener = keyboard.Listener(on_press=on_press)‚Ä®listener.start()
At the start of the question-trigger block, check:
if not ARMED:‚Ä®    continue
üéß (Optional) Listen only to the other party (Pro)
If you don‚Äôt want your own voice to trigger, on macOS use a virtual audio device such as BlackHole / Loopback to feed the meeting app‚Äôs output into this program:
System audio output = BlackHole
Meet/Zoom/Teams output = BlackHole
In the app, select BlackHole input with --device
(Optionally use Multi-Output to also send the same sound to your headphones)
‚úÖ Quick Checklist
.env ‚Üí OPENAI_API_KEY=...
Found microphone index with waa --list-devices
If needed, set require_dev_keyword: true + edit keywords.en.txt
(Optional) Adjust min_lang_prob, min_chars thresholds
Run: (choose one of the following)
python -m waa.cli --whisper-model base --device <INDEX>‚Ä®# or‚Ä®./.venv/bin/waa --whisper-model base --device <INDEX>
